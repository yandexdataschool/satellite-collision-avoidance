{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### От Ирины\n",
    "\n",
    "**Расчет маневров при действительно опасном сближении.**\n",
    "\n",
    "Если зафиксировано опасное сближение (ориентировочно до 100 м), то задача ИИ – определить параметры маневра для «защищаемого» объекта, чтобы он ушел от столкновения с целью минимизировать «потери». С точки зрения баллистики в «потери» я могу включить только затраты топлива. С точки зрения BigData в «потерях» можно учитывать много дополнительных данных – насколько много у аппарата останется топлива, помешает ли ему этот маневр выполнить какую-то запланированную целевую задачу (например, съемку определенного района) и т.д. Таким образом, задача ИИ – найти оптимальные (по величине этого функционала) параметры маневра, спасающего аппарат от опасного сближения. При этом дополнительно нужно проверять, что при этом маневре у «защищаемого» объекта не возникнет опасных сближений с какими-то другими посторонними объектами из базы.\n",
    "В какой форме записать функционал – отдельная системная задача, где нужен мозговой штурм. Нужно ли сводить все разнообразные параметры в единую функцию с весовыми коэффициентами типа\n",
    "\n",
    "$$\n",
    "Ф=α1*ΔV+ α2*\\text{флаг(не испортили маневром временной интервал выполнения съемки: 0 или 1)}+…+ αn*\\text{[n-й критерий]}\n",
    "$$\n",
    "\n",
    "Либо все же распределить параметры на несколько функционалов, чтобы ИИ выдавал табличкой перечень найденных вариантов маневров с различными значениями по различным критериям, а окончательное решение принимает уже оператор или группа управления.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_xyz(coordinates):\n",
    "    \"\"\"\n",
    "    ф-я переводит координаты в xyz\n",
    "    (пока случайно)\n",
    "    \"\"\"\n",
    "    result = np.random.normal(size=[coordinates.shape[0], 3])\n",
    "    return result\n",
    "    \n",
    "class API:\n",
    "    \n",
    "    def get_reward(self, state, next_state, current_reward):\n",
    "        \"\"\"\n",
    "        state, next_state: dict where keys:\n",
    "            'coordinates': dict where:\n",
    "                {'st': np.array shape (n_satellites, 6)}:  satellites coordinates\n",
    "                {'gb': np.array shape (n_items, 6)}:  garbage coordinates\n",
    "            'fuel': float\n",
    "            'trajectory_deviation_coef': float < 10\n",
    "        current_reward: float\n",
    "        ---\n",
    "        output: float\n",
    "        \"\"\"\n",
    "        # координаты КА\n",
    "        satellites_coordinates = to_xyz(state['coordinates']['st'])\n",
    "        # координаты остальных объектов\n",
    "        items_state = to_xyz(state['coordinates']['gb'])\n",
    "\n",
    "        # rewards\n",
    "        # за сближение (просто считаем Евклидово расстояние между\n",
    "        # КА и другим объектом) и экспоненту\n",
    "        collision_danger = 0\n",
    "        for j in range(satellites_coordinates.shape[0]):\n",
    "            for i in range(items_state.shape[0]):\n",
    "                # Евклидово расстояние\n",
    "                item_danger = np.sum((satellites_coordinates[j] - items_state[i]) ** 2) ** 0.5\n",
    "                # \n",
    "                item_danger = 1. / item_danger\n",
    "                print 'item danger: ', item_danger\n",
    "                collision_danger -= item_danger\n",
    "        print 'collision danger: ', collision_danger\n",
    "        \n",
    "        # штраф за расход топлива  \n",
    "        fuel_consumption = -(state['fuel'] - next_state['fuel'])\n",
    "        print 'fuel consumption:', fuel_consumption\n",
    "        \n",
    "        # награда за движение близко к траектории\n",
    "        traj_reward = -state['trajectory_deviation_coef']\n",
    "        print 'traj reward:', traj_reward\n",
    "        \n",
    "        # whole reward    \n",
    "        reward = (\n",
    "            collision_danger\n",
    "            + fuel_consumption\n",
    "            + traj_reward\n",
    "        ) \n",
    "        #^-- остальные rewards. можно с весом суммировать или в степени возволить\n",
    "        \n",
    "        new_reward = current_reward + reward\n",
    "        print 'reward:', new_reward\n",
    "        return new_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state = {\n",
    "    'coordinates': {\n",
    "        'st': np.random.normal(size=[2, 6]),\n",
    "        'gb': np.random.normal(size=[20, 6])\n",
    "    },\n",
    "    'fuel': 10,\n",
    "    'trajectory_deviation_coef': 10,\n",
    "}\n",
    "next_state = {\n",
    "    'coordinates': {\n",
    "        'st': np.random.normal(size=[2, 6]),\n",
    "        'gb': np.random.normal(size=[20, 6])\n",
    "    },\n",
    "    'fuel': 8,\n",
    "    'trajectory_deviation_coef': 8,\n",
    "}\n",
    "current_reward = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item danger:  0.498466745071\n",
      "item danger:  0.505707962511\n",
      "item danger:  0.345736754166\n",
      "item danger:  0.538586154379\n",
      "item danger:  0.491386171993\n",
      "item danger:  0.458367971405\n",
      "item danger:  0.396947214784\n",
      "item danger:  0.547117841118\n",
      "item danger:  0.294715039125\n",
      "item danger:  0.396861904869\n",
      "item danger:  0.403445869308\n",
      "item danger:  0.393570111174\n",
      "item danger:  0.376394098979\n",
      "item danger:  0.716307281698\n",
      "item danger:  0.774957211384\n",
      "item danger:  0.688528069775\n",
      "item danger:  0.397535478441\n",
      "item danger:  0.689358859187\n",
      "item danger:  0.321259805633\n",
      "item danger:  0.666872549284\n",
      "item danger:  0.452366152774\n",
      "item danger:  0.502126361898\n",
      "item danger:  0.381115460318\n",
      "item danger:  0.632207348333\n",
      "item danger:  0.389563920871\n",
      "item danger:  0.530615013332\n",
      "item danger:  0.498194841765\n",
      "item danger:  1.08947185479\n",
      "item danger:  0.271015720709\n",
      "item danger:  0.389370266896\n",
      "item danger:  0.594968473303\n",
      "item danger:  0.416797689912\n",
      "item danger:  0.364747020107\n",
      "item danger:  0.475987250756\n",
      "item danger:  0.438222392841\n",
      "item danger:  0.60465638709\n",
      "item danger:  0.620688028786\n",
      "item danger:  0.665693592367\n",
      "item danger:  0.339803379579\n",
      "item danger:  0.407968646442\n",
      "collision danger:  -19.9677028972\n",
      "fuel consumption: -2\n",
      "traj reward: -10\n",
      "reward: -21.9677028972\n"
     ]
    }
   ],
   "source": [
    "#проверка\n",
    "A = API()\n",
    "B = A.get_reward(state, next_state, current_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**вопросы**\n",
    "\n",
    "1. проверить - reward за state или action? предыдущий state или следующий как второй брать\n",
    "1. метод get_reward должен на входе еще иметь параметры - пороги всякие и прочее или все функциями с этими параметрами сделать\n",
    "1. ф-я to_xyz\n",
    "1. коэф отклонения от траектории\n",
    "1. вместо времени жизни давайте введем коэф отклонения от траектории. Когда отклонение мало пусть награду получает.\n",
    "1. заканчивать сессию когда кончается топливо\n",
    "\n",
    "**потом**\n",
    "\n",
    "* чтобы натренить модель можно сделать и натренить несколько \"охотников\", которые будут старатья сбить защищаемые объет. можно, например, разрешать им редко менять трактории, что они впрямую не летали за объектом, или скорость меньше задать.\n",
    "* сделать нейронку (или что-то другое), которая будет говорить, какие объекты потенциально опасные. и потом можно типа там 100000 объектов назадавать, чтоб училась и смотрелось нормально. охотниками пусть случайные объекты выбираются через какое-то время\n",
    "* штрафовать за большой угол\n",
    "\n",
    "**визуализация**\n",
    "\n",
    "* 2D\n",
    "* подсвечивать маневры и отклонение от траектории\n",
    "* подсвечивать охотников\n",
    "\n",
    "**RL**\n",
    "\n",
    "* encoder для state\n",
    "\n",
    "**мысли**\n",
    "\n",
    "можно reward:\n",
    "+ за время без маневров\n",
    "+ за время летения по правильной траектории\n",
    "- за топливо\n",
    "- за маневр\n",
    "-!!! за столкновение\n",
    "\n",
    "можно еще\n",
    "+ за время без опасности\n",
    "- за опасность\n",
    "но, вроде, т.к. опасность => маневр, то штраф за маневр и время\n",
    "без маневров это учитывают\n",
    "\n",
    "ну или, кстати, можно еще за сближение <R штрафовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
