{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pykep as pk\n",
    "\n",
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from api import Environment, MAX_FUEL_CONSUMPTION\n",
    "from simulator import Simulator, read_space_objects\n",
    "from agent import TableAgent as Agent\n",
    "\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 6599.95\n",
    "step = 0.0001\n",
    "end_time = 6600.05\n",
    "\n",
    "n_actions = 3\n",
    "\n",
    "osc = read_space_objects(parent_dir + \"/data/collision.osc\", \"osc\")\n",
    "protected = osc[0]\n",
    "debris = [osc[1]]\n",
    "\n",
    "start_time_mjd2000 = pk.epoch(start_time, \"mjd2000\")\n",
    "max_fuel_cons = MAX_FUEL_CONSUMPTION\n",
    "fuel_level = protected.get_fuel()\n",
    "\n",
    "env = Environment(copy(protected), copy(debris), start_time_mjd2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_shape = (10, ) # debr and protected positions + fuel?\n",
    "action_shape = 4\n",
    "n_actions = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actions_dict = {\n",
    "    0: np.array([0.1, 0, 0, 0.01]),\n",
    "    1: np.array([0, 0.1, 0, 0.01]),\n",
    "    2: np.array([0, 0, 0.1, 0.01]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Defining a network\n",
    "\n",
    "With all it's complexity, at it's core TRPO is yet another policy gradient method. \n",
    "\n",
    "This essentially means we're actually training a stochastic policy $ \\pi_\\theta(a|s) $. \n",
    "\n",
    "And yes, it's gonna be a neural network. So let's start by defining one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T11:39:23.631230Z",
     "start_time": "2017-08-24T11:39:23.627975Z"
    }
   },
   "outputs": [],
   "source": [
    "# import necessary building blocks\n",
    "from keras.models import Sequential\n",
    "# from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout\n",
    "import keras.layers as L\n",
    "from keras.layers.advanced_activations import LeakyReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input tensors\n",
    "observations_ph = tf.placeholder(shape=(None, observation_shape[0]), dtype=tf.float32)\n",
    "# Actions that we made\n",
    "actions_ph = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "# \"G = r + gamma*r' + gamma^2*r'' + ...\"\n",
    "cummulative_returns_ph = tf.placeholder(shape=(None,), dtype=tf.float32) \n",
    "# Action probabilities from previous iteration\n",
    "old_probs_ph =  tf.placeholder(shape=(None, n_actions), dtype=tf.float32)\n",
    "\n",
    "all_inputs = [observations_ph,actions_ph,cummulative_returns_ph,old_probs_ph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denselayer(name, x, out_dim, nonlinearity=None):\n",
    "    with tf.variable_scope(name):\n",
    "        if nonlinearity is None:\n",
    "            nonlinearity = tf.identity\n",
    "\n",
    "        x_shape = x.get_shape().as_list()\n",
    "\n",
    "        w = tf.get_variable('w', shape=[x_shape[1], out_dim])\n",
    "        b = tf.get_variable('b', shape=[out_dim], initializer=tf.constant_initializer(0))\n",
    "        o = nonlinearity(tf.matmul(x, w) + b)\n",
    "\n",
    "        return o\n",
    "\n",
    "# TODO - awesome NN + more cosmic features\n",
    "sess = tf.InteractiveSession()\n",
    "nn = Sequential([\n",
    "    L.InputLayer(input_shape=(observation_shape)),\n",
    "    L.Dense(64, activation='relu'),\n",
    "    L.Dense(n_actions, activation=tf.nn.log_softmax),\n",
    "    \n",
    "])\n",
    "\n",
    "'''!!! проверить'''\n",
    "policy_out = nn(observations_ph)\n",
    "\n",
    "probs_out = tf.exp(policy_out)\n",
    "\n",
    "weights = tf.trainable_variables()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Actions and rollouts\n",
    "\n",
    "In this section, we'll define functions that take actions $ a \\sim \\pi_\\theta(a|s) $ and rollouts $ <s_0,a_0,s_1,a_1,s_2,a_2,...s_n,a_n> $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compile function\n",
    "\n",
    "def act(obs, sample=True):\n",
    "    \"\"\"\n",
    "    Samples action from policy distribution (sample = True) or takes most likely action (sample = False)\n",
    "    :param: obs - single observation vector\n",
    "    :param sample: if True, samples from \\pi, otherwise takes most likely action\n",
    "    :returns: action (single integer) and probabilities for all actions\n",
    "    \"\"\"\n",
    "    '''!!! добавить время какое-то в obs?'''\n",
    "    '''!!! тут должно быть распределение'''\n",
    "    probs = sess.run(probs_out, feed_dict = {observations_ph:obs.reshape((1, -1))})[0]\n",
    "\n",
    "    if sample:\n",
    "        action = int(np.random.choice(n_actions,p=probs))\n",
    "    else:\n",
    "        action = int(np.argmax(probs))\n",
    "\n",
    "    return action, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one action:\n",
      " (1, array([ 0.33363914,  0.43575013,  0.2306108 ], dtype=float32))\n",
      "\n",
      "sampled:\n",
      " [(2, array([ 0.33363914,  0.43575013,  0.2306108 ], dtype=float32)), (2, array([ 0.33363914,  0.43575013,  0.2306108 ], dtype=float32)), (0, array([ 0.33363914,  0.43575013,  0.2306108 ], dtype=float32)), (0, array([ 0.33363914,  0.43575013,  0.2306108 ], dtype=float32)), (0, array([ 0.33363914,  0.43575013,  0.2306108 ], dtype=float32))]\n",
      "\n",
      "greedy:\n",
      " [(1, array([ 0.33363914,  0.43575013,  0.2306108 ], dtype=float32)), (1, array([ 0.33363914,  0.43575013,  0.2306108 ], dtype=float32)), (1, array([ 0.33363914,  0.43575013,  0.2306108 ], dtype=float32)), (1, array([ 0.33363914,  0.43575013,  0.2306108 ], dtype=float32)), (1, array([ 0.33363914,  0.43575013,  0.2306108 ], dtype=float32))]\n"
     ]
    }
   ],
   "source": [
    "#demo\n",
    "demo_obs = np.random.normal(size=(10,))\n",
    "'''!!! тут должно быть распределение'''\n",
    "print (\"one action:\\n\", act(demo_obs))\n",
    "print (\"\\nsampled:\\n\", [act(demo_obs) for _ in range(5)])\n",
    "print (\"\\ngreedy:\\n\", [act(demo_obs,sample=False) for _ in range(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute cummulative reward just like you did in vanilla REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "def get_cummulative_returns(r, gamma=1):\n",
    "    \"\"\"\n",
    "    Computes cummulative discounted rewards given immediate rewards\n",
    "    G_i = r_i + gamma*r_{i+1} + gamma^2*r_{i+2} + ...\n",
    "    Also known as R(s,a).\n",
    "    \"\"\"\n",
    "    r = np.array(r)\n",
    "    assert r.ndim >= 1\n",
    "    return scipy.signal.lfilter([1], [1, -gamma], r[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.40049,  1.5561 ,  1.729  ,  0.81   ,  0.9    ,  1.     ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#simple demo on rewards [0,0,1,0,0,1]\n",
    "get_cummulative_returns([0,0,1,0,0,1],gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rollout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rollout(env, act, max_pathlength=2500, n_timesteps=50000):\n",
    "    \"\"\"\n",
    "    Generate rollouts for training.\n",
    "    :param: env - environment in which we will make actions to generate rollouts.\n",
    "    :param: act - the function that can return policy and action given observation.\n",
    "    :param: max_pathlength - maximum size of one path that we generate.\n",
    "    :param: n_timesteps - total sum of sizes of all pathes we generate.\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "\n",
    "    total_timesteps = 0\n",
    "    while total_timesteps < n_timesteps:\n",
    "        obervations, actions, rewards, action_probs = [], [], [], []\n",
    "        obervation = env.reset()\n",
    "        for _ in range(max_pathlength):\n",
    "            action, policy = act(obervation)\n",
    "            obervations.append(obervation)\n",
    "            actions.append(action)\n",
    "            action_probs.append(policy)\n",
    "            obervation, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            total_timesteps += 1\n",
    "            if done or total_timesteps==n_timesteps:\n",
    "                path = {\"observations\": np.array(obervations),\n",
    "                        \"policy\": np.array(action_probs),\n",
    "                        \"actions\": np.array(actions),\n",
    "                        \"rewards\": np.array(rewards),\n",
    "                        \"cumulative_returns\":get_cummulative_returns(rewards),\n",
    "                       }\n",
    "                paths.append(path)\n",
    "                break\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'policy': array([[ 0.38928664,  0.33861947,  0.27209392],\n",
      "       [ 0.39049041,  0.33795345,  0.27155608],\n",
      "       [ 0.38491449,  0.34159634,  0.27348921],\n",
      "       [ 0.38708171,  0.33602256,  0.27689573],\n",
      "       [ 0.39258957,  0.32314593,  0.28426448]], dtype=float32), 'cumulative_returns': array([-5., -4., -3., -2., -1.]), 'rewards': array([-1., -1., -1., -1., -1.]), 'observations': array([[ 0.99662689,  0.0820661 ,  0.99869615, -0.05104893, -0.05392737,\n",
      "         0.00220854],\n",
      "       [ 0.99814499,  0.06088161,  0.99927089, -0.0381797 , -0.15365352,\n",
      "         0.12096028],\n",
      "       [ 0.9999474 ,  0.01025645,  0.99960799,  0.02799744, -0.3403785 ,\n",
      "         0.52118779],\n",
      "       [ 0.99912625, -0.04179392,  0.99593733,  0.09004905, -0.16809848,\n",
      "         0.08245382],\n",
      "       [ 0.99854508, -0.0539233 ,  0.99833912,  0.05761076,  0.04788444,\n",
      "        -0.40352728]]), 'actions': array([1, 2, 0, 0, 1])}\n",
      "It's ok\n"
     ]
    }
   ],
   "source": [
    "paths = rollout(env,act,max_pathlength=5,n_timesteps=100)\n",
    "print paths[-1]\n",
    "assert (paths[0]['policy'].shape==(5, n_actions))\n",
    "assert (paths[0]['cumulative_returns'].shape==(5,))\n",
    "assert (paths[0]['rewards'].shape==(5,))\n",
    "assert (paths[0]['observations'].shape==(5,)+observation_shape)\n",
    "assert (paths[0]['actions'].shape==(5,))\n",
    "print ('It\\'s ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: loss functions\n",
    "\n",
    "Now let's define the loss functions and constraints for actual TRPO training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The surrogate reward should be\n",
    "$$J_{surr}=E_{a \\sim \\pi_{old}} \\Big[\\sum\\limits_{i=0}^N \\frac{\\pi_{\\theta}(s_i, a_i)}{\\pi_{\\theta_{old}}(s_i, a_i)}A_{\\theta_{old}(s_i, a_i)}\\Big]$$\n",
    "\n",
    "For simplicity, let's use cummulative returns instead of advantage for now:\n",
    "$$J'_{surr}=E_{a \\sim \\pi_{old}} \\Big[\\sum\\limits_{i=0}^N \\frac{\\pi_{\\theta}(s_i, a_i)}{\\pi_{\\theta_{old}}(s_i, a_i)}R_{\\theta_{old}(s_i, a_i)}\\Big]$$\n",
    "\n",
    "Or alternatively, minimize the surrogate loss:\n",
    "$$ L_{surr} = - J'_{surr} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select probabilities of chosen actions\n",
    "batch_size = tf.shape(observations_ph)[0]\n",
    "probs_all = tf.reshape(probs_out, [-1])\n",
    "probs_for_actions = tf.gather(probs_all, tf.range(0, batch_size) * n_actions + actions_ph)\n",
    "old_probs_all = tf.reshape(old_probs_ph, [-1])\n",
    "old_probs_for_actions = tf.gather(old_probs_all, tf.range(0, batch_size) * n_actions + actions_ph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute surrogate loss, aka importance-sampled policy gradient\n",
    "'''\n",
    "тоже я что-то пишу (+там формула другая стала)\n",
    "'''\n",
    "L_surr = - tf.reduce_mean(probs_for_actions / old_probs_for_actions * cummulative_returns_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "#compute and return surrogate policy gradient\n",
    "def var_shape(x):\n",
    "    return [k.value for k in x.get_shape()]\n",
    "\n",
    "def numel(x):\n",
    "    return np.prod(var_shape(x))\n",
    "\n",
    "def flatgrad(loss, var_list):\n",
    "    grads = tf.gradients(loss, var_list)\n",
    "    return tf.concat([tf.reshape(grad, [numel(v)])\n",
    "                      for (v, grad) in zip(var_list, grads)], 0)\n",
    "\n",
    "flat_grad_surr = flatgrad(L_surr, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ascend these gradients as long as our $pi_\\theta(a|s)$ satisfies the constraint\n",
    "$$E_{s,\\pi_{\\Theta_{t}}}\\Big[KL(\\pi(\\Theta_{t}, s) \\:||\\:\\pi(\\Theta_{t+1}, s))\\Big]< \\alpha$$\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "$$KL(p||q) = E _p log({p \\over q})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute Kullback-Leibler divergence (see formula above)\n",
    "old_log_probs = tf.log(old_probs_ph+1e-10)\n",
    "\n",
    "# kl = <cumpute kullback-leibler as per formula above>\n",
    "kl = tf.reduce_mean(tf.reduce_sum(old_probs_ph * (old_log_probs - policy_out), axis=1))\n",
    "\n",
    "#Compute policy entropy\n",
    "# entropy = <compute policy entropy. Don't forget the sign!>\n",
    "entropy = - tf.reduce_mean(tf.reduce_sum(policy_out * probs_out, axis=1))\n",
    "\n",
    "losses = [L_surr, kl, entropy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear search**\n",
    "\n",
    "TRPO in its core involves ascending surrogate policy gradient constrained by KL divergence. \n",
    "\n",
    "In order to enforce this constraint, we're gonna use linesearch. You can find out more about it [here](https://en.wikipedia.org/wiki/Linear_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linesearch(f, x, fullstep, max_kl):\n",
    "    \"\"\"\n",
    "    Linesearch finds the best parameters of neural networks in the direction of fullstep contrainted by KL divergence.\n",
    "    :param: f - function that returns loss, kl and arbitrary third component.\n",
    "    :param: x - old parameters of neural network.\n",
    "    :param: fullstep - direction in which we make search.\n",
    "    :param: max_kl - constraint of KL divergence.\n",
    "    :returns:\n",
    "    \"\"\"\n",
    "    max_backtracks = 10\n",
    "    loss, _, _ = f(x)\n",
    "    for stepfrac in .5**np.arange(max_backtracks):\n",
    "        xnew = x + stepfrac * fullstep\n",
    "        new_loss, kl, _ = f(xnew)\n",
    "        actual_improve = new_loss - loss\n",
    "        if kl<=max_kl and actual_improve < 0:\n",
    "            x = xnew\n",
    "            loss = new_loss\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: training\n",
    "In this section we construct rest parts of our computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slice_vector(vector, shapes):\n",
    "    \"\"\"\n",
    "    Slices symbolic vector into several symbolic tensors of given shapes.\n",
    "    Auxilary function used to un-flatten gradients, tangents etc.\n",
    "    :param vector: 1-dimensional symbolic vector\n",
    "    :param shapes: list or tuple of shapes (list, tuple or symbolic)\n",
    "    :returns: list of symbolic tensors of given shapes\n",
    "    \"\"\"\n",
    "    assert len(vector.get_shape())==1,\"vector must be 1-dimensional\"\n",
    "    start = 0\n",
    "    tensors = []\n",
    "    for shape in shapes:\n",
    "        size = np.prod(shape)\n",
    "        tensor = tf.reshape(vector[start:(start + size)],shape)\n",
    "        tensors.append(tensor)\n",
    "        start += size\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#intermediate grad in conjugate_gradient\n",
    "conjugate_grad_intermediate_vector = tf.placeholder(dtype=tf.float32, shape=(None,))\n",
    "\n",
    "#slice flat_tangent into chunks for each weight\n",
    "weight_shapes = [sess.run(var).shape for var in weights]\n",
    "tangents = slice_vector(conjugate_grad_intermediate_vector,weight_shapes)\n",
    "\n",
    "# KL divergence where first arg is fixed\n",
    "kl_firstfixed = tf.reduce_sum((tf.stop_gradient(probs_out) * (tf.stop_gradient(tf.log(probs_out)) - tf.log(probs_out))))/ tf.cast(batch_size, tf.float32)\n",
    "\n",
    "#compute fisher information matrix (used for conjugate gradients and to estimate KL)\n",
    "gradients = tf.gradients(kl_firstfixed, weights)\n",
    "gradient_vector_product = [tf.reduce_sum(g[0] * t) for (g, t) in zip(gradients, tangents)]\n",
    "\n",
    "fisher_vec_prod = flatgrad(gradient_vector_product, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRPO helpers\n",
    "\n",
    "Here we define a few helper functions used in the main TRPO loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conjugate gradients**\n",
    "\n",
    "Since TRPO includes contrainted optimization, we will need to solve Ax=b using conjugate gradients.\n",
    "\n",
    "In general, CG is an algorithm that solves Ax=b where A is positive-defined. A is Hessian matrix so A is positive-defined. You can find out more about them [here](https://en.wikipedia.org/wiki/Conjugate_gradient_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "def conjugate_gradient(f_Ax, b, cg_iters=10, residual_tol=1e-10):\n",
    "    \"\"\"\n",
    "    This method solves system of equation Ax=b using iterative method called conjugate gradients\n",
    "    :f_Ax: function that returns Ax\n",
    "    :b: targets for Ax\n",
    "    :cg_iters: how many iterations this method should do\n",
    "    :residual_tol: epsilon for stability\n",
    "    \"\"\"\n",
    "    p = b.copy()\n",
    "    r = b.copy()\n",
    "    x = np.zeros_like(b)\n",
    "    rdotr = r.dot(r)\n",
    "    for i in range(cg_iters):\n",
    "        z = f_Ax(p)\n",
    "        v = rdotr / (p.dot(z) + 1e-8)\n",
    "        x += v * p\n",
    "        r -= v * z\n",
    "        newrdotr = r.dot(r)\n",
    "        mu = newrdotr / (rdotr + 1e-8)\n",
    "        p = r + mu * p\n",
    "        rdotr = newrdotr\n",
    "        if rdotr < residual_tol:\n",
    "            break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.41690429 -1.58069313  1.83182184 -2.37012842  0.66060829 -0.25352844\n",
      " -0.7793311   0.71620164]\n",
      "[ 1.41680435 -1.58051343  1.83184162 -2.3701266   0.66057381 -0.25350833\n",
      " -0.77925583  0.71615073]\n"
     ]
    }
   ],
   "source": [
    "#This code validates conjugate gradients\n",
    "A = np.random.rand(8, 8)\n",
    "A = np.matmul(np.transpose(A), A)\n",
    "\n",
    "def f_Ax(x):\n",
    "    return np.matmul(A, x.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "b = np.random.rand(8)\n",
    "\n",
    "w = np.matmul(np.matmul(inv(np.matmul(np.transpose(A), A)), np.transpose(A)), b.reshape((-1, 1))).reshape(-1)\n",
    "print (w)\n",
    "print (conjugate_gradient(f_Ax, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compile a function that exports network weights as a vector\n",
    "flat_weights = tf.concat([tf.reshape(var, [-1]) for var in weights], axis=0)\n",
    "\n",
    "#... and another function that imports vector back into network weights\n",
    "flat_weights_placeholder = tf.placeholder(tf.float32, shape=(None,))\n",
    "assigns = slice_vector(flat_weights_placeholder, weight_shapes)\n",
    "\n",
    "load_flat_weights = [w.assign(ph) for w, ph in zip(weights, assigns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Main TRPO loop\n",
    "\n",
    "Here we will train our network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** Iteration 1 ************\n",
      "Rollout\n",
      "Made rollout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of episodes:                 66\n",
      "Average sum of rewards per episode:       -756.590909091\n",
      "Std of rewards per episode:               264.844664437\n",
      "Entropy:                                  1.03407\n",
      "Time elapsed:                             0.61 mins\n",
      "KL between old and new distribution:      -1.7214e-08\n",
      "Surrogate loss:                           424.597\n",
      "\n",
      "********** Iteration 2 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 126\n",
      "Average sum of rewards per episode:       -832.35\n",
      "Std of rewards per episode:               356.566909336\n",
      "Entropy:                                  1.0339\n",
      "Time elapsed:                             1.10 mins\n",
      "KL between old and new distribution:      -1.69948e-08\n",
      "Surrogate loss:                           492.467\n",
      "\n",
      "********** Iteration 3 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 184\n",
      "Average sum of rewards per episode:       -861.086206897\n",
      "Std of rewards per episode:               328.257430753\n",
      "Entropy:                                  1.00796\n",
      "Time elapsed:                             1.64 mins\n",
      "KL between old and new distribution:      0.00975528\n",
      "Surrogate loss:                           493.029\n",
      "\n",
      "********** Iteration 4 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 224\n",
      "Average sum of rewards per episode:       -1061.525\n",
      "Std of rewards per episode:               450.4360103\n",
      "Entropy:                                  1.01452\n",
      "Time elapsed:                             2.17 mins\n",
      "KL between old and new distribution:      -1.70259e-08\n",
      "Surrogate loss:                           626.254\n",
      "\n",
      "********** Iteration 5 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 272\n",
      "Average sum of rewards per episode:       -988.604166667\n",
      "Std of rewards per episode:               429.020043218\n",
      "Entropy:                                  0.97541\n",
      "Time elapsed:                             2.68 mins\n",
      "KL between old and new distribution:      0.00996863\n",
      "Surrogate loss:                           586.366\n",
      "\n",
      "********** Iteration 6 ************\n",
      "Rollout\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-6bd7ca80d209>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Generating paths.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rollout\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Made rollout\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5fddefe1fe18>\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(env, act, max_pathlength, n_timesteps)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mobervation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_pathlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobervation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mobervations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobervation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-fcbfba8d916a>\u001b[0m in \u001b[0;36mact\u001b[0;34m(obs, sample)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mobservations_ph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dev/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dev/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dev/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dev/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dev/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from itertools import count\n",
    "from collections import OrderedDict\n",
    "\n",
    "max_kl=0.1           #this is hyperparameter of TRPO. It controls how big KL divergence may be between old and new policy every step.\n",
    "max_kl=0.01           #this is hyperparameter of TRPO. It controls how big KL divergence may be between old and new policy every step.\n",
    "cg_damping=0.1        #This parameters regularize addition to\n",
    "numeptotal = 0        #this is number of episodes that we played.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in count(1):\n",
    "\n",
    "    print (\"\\n********** Iteration %i ************\" % i)\n",
    "\n",
    "    # Generating paths.\n",
    "    print(\"Rollout\")\n",
    "    paths = rollout(env,act)\n",
    "    print (\"Made rollout\")\n",
    "    \n",
    "    # Updating policy.\n",
    "    observations = np.concatenate([path[\"observations\"] for path in paths])\n",
    "    actions = np.concatenate([path[\"actions\"] for path in paths])\n",
    "    returns = np.concatenate([path[\"cumulative_returns\"] for path in paths])\n",
    "    old_probs = np.concatenate([path[\"policy\"] for path in paths])\n",
    "    inputs_batch=[observations,actions,returns,old_probs]\n",
    "    feed_dict = {observations_ph:observations,\n",
    "                 actions_ph:actions,\n",
    "                 old_probs_ph:old_probs,\n",
    "                 cummulative_returns_ph:returns,\n",
    "                }\n",
    "    old_weights = sess.run(flat_weights)\n",
    "    \n",
    "    def fisher_vector_product(p):\n",
    "        \"\"\"gets intermediate grads (p) and computes fisher*vector \"\"\"\n",
    "        feed_dict[conjugate_grad_intermediate_vector] = p\n",
    "        return sess.run(fisher_vec_prod, feed_dict) + cg_damping * p\n",
    "\n",
    "    flat_grad = sess.run(flat_grad_surr, feed_dict)\n",
    "    \n",
    "    stepdir = conjugate_gradient(fisher_vector_product, -flat_grad)\n",
    "    shs = .5 * stepdir.dot(fisher_vector_product(stepdir))\n",
    "    lm = np.sqrt(shs / max_kl)\n",
    "    fullstep = stepdir / lm\n",
    "    \n",
    "    #Compute new weights with linesearch in the direction we found with CG\n",
    "    \n",
    "    def losses_f(flat_weights):\n",
    "        feed_dict[flat_weights_placeholder] = flat_weights\n",
    "        sess.run(load_flat_weights, feed_dict)\n",
    "        return sess.run(losses, feed_dict)\n",
    "\n",
    "    new_weights = linesearch(losses_f, old_weights, fullstep, max_kl)\n",
    "    feed_dict[flat_weights_placeholder] = new_weights\n",
    "    sess.run(load_flat_weights, feed_dict)\n",
    "\n",
    "    #Report current progress\n",
    "    L_surr, kl, entropy = sess.run(losses, feed_dict)\n",
    "    episode_rewards = np.array([path[\"rewards\"].sum() for path in paths])\n",
    "\n",
    "    stats = OrderedDict()\n",
    "    numeptotal += len(episode_rewards)\n",
    "    stats[\"Total number of episodes\"] = numeptotal\n",
    "    stats[\"Average sum of rewards per episode\"] = episode_rewards.mean()\n",
    "    stats[\"Std of rewards per episode\"] = episode_rewards.std()\n",
    "    stats[\"Entropy\"] = entropy\n",
    "    stats[\"Time elapsed\"] = \"%.2f mins\" % ((time.time() - start_time)/60.)\n",
    "    stats[\"KL between old and new distribution\"] = kl\n",
    "    stats[\"Surrogate loss\"] = L_surr\n",
    "    for k, v in stats.items():\n",
    "        print(k + \": \" + \" \" * (40 - len(k)) + str(v))\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework option I: better sampling (10+pts)\n",
    "\n",
    "In this section, you're invited to implement a better rollout strategy called _vine_.\n",
    "\n",
    "![img](https://s17.postimg.org/i90chxgvj/vine.png)\n",
    "\n",
    "In most gym environments, you can actually backtrack by using states. You can find a wrapper that saves/loads states in [the mcts seminar](https://github.com/yandexdataschool/Practical_RL/blob/master/yet_another_week/seminar_MCTS.ipynb).\n",
    "\n",
    "You can read more about in the [TRPO article](https://arxiv.org/abs/1502.05477) in section 5.2.\n",
    "\n",
    "The goal here is to implement such rollout policy (we recommend using tree data structure like in the seminar above).\n",
    "Then you can assign cummulative rewards similar to `get_cummulative_rewards`, but for a tree.\n",
    "\n",
    "__bonus task__ - parallelize samples using multiple cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework option II (10+pts)\n",
    "\n",
    "Let's use TRPO to train evil robots! (pick any of two)\n",
    "* [MuJoCo robots](https://gym.openai.com/envs#mujoco)\n",
    "* [Box2d robot](https://gym.openai.com/envs/BipedalWalker-v2)\n",
    "\n",
    "The catch here is that those environments have continuous action spaces. \n",
    "\n",
    "Luckily, TRPO is a policy gradient method, so it's gonna work for any parametric $\\pi_\\theta(a|s)$. We recommend starting with gaussian policy:\n",
    "\n",
    "$$\\pi_\\theta(a|s) = N(\\mu_\\theta(s),\\sigma^2_\\theta(s)) = {1 \\over \\sqrt { 2 \\pi {\\sigma^2}_\\theta(s) } } e^{ (a - \n",
    "\\mu_\\theta(s))^2 \\over 2 {\\sigma^2}_\\theta(s) } $$\n",
    "\n",
    "In the $\\sqrt { 2 \\pi {\\sigma^2}_\\theta(s) }$ clause, $\\pi$ means ~3.1415926, not agent's policy.\n",
    "\n",
    "This essentially means that you will need two output layers:\n",
    "* $\\mu_\\theta(s)$, a dense layer with linear activation\n",
    "* ${\\sigma^2}_\\theta(s)$, a dense layer with activation tf.exp (to make it positive; like rho from bandits)\n",
    "\n",
    "For multidimensional actions, you can use fully factorized gaussian (basically a vector of gaussians).\n",
    "\n",
    "__bonus task__: compare performance of continuous action space method to action space discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py333",
   "language": "python",
   "name": "py333"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
