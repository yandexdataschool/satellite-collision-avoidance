{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space Navigator with CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from space_navigator.models.CE import CrossEntropy\n",
    "from space_navigator.models.baseline import Baseline\n",
    "from space_navigator.utils import read_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = \"../../../data/environments/collision.env\"\n",
    "env = read_environment(env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Agrs:\n",
      "            env (Environment): environment with given parameteres.\n",
      "            step (float): time step in simulation.\n",
      "            reverse (bool): if True, there are selected exactly 2 maneuvers\n",
      "                while the second of them is reversed to the first one.\n",
      "            first_maneuver_time (str): time to the first maneuver. Could be:\n",
      "                \"early\": max time to the first maneuver, namely\n",
      "                    max(0, 0.5, 1.5, 2.5 ... orbital_periods before collision);\n",
      "                \"auto\".\n",
      "            n_maneuvers (int): total number of maneuvers.\n",
      "            lr (float): learning rate for stability.\n",
      "            percentile_growth (float): coefficient of changing percentile.\n",
      "            sigma_dV, sigma_t (float): sigma of dV and sigma of time_to_req.\n",
      "                If None, the values are calculated automatically.\n",
      "\n",
      "        TODO:\n",
      "            path to save plots.\n",
      "            variable step propagation step.\n",
      "\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# __init__ docstring\n",
    "print(CrossEntropy.__init__.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_parameters = {\n",
    "    \"env\": env,\n",
    "    \"step\": 1e-6,\n",
    "    \"reverse\": True, \n",
    "    \"first_maneuver_time\": 'early',\n",
    "    \"n_maneuvers\": 2,\n",
    "    \"lr\": 0.7,\n",
    "    \"percentile\": 80,\n",
    "    \"sigma_dV\": None,\n",
    "    \"sigma_t\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CE_model = CrossEntropy(**init_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration.\n",
      "\n",
      "        Args:\n",
      "            print_out (bool): print iteration information.\n",
      "            n_sessions (int): number of sessions per iteration.\n",
      "            sigma_decay (float): coefficient of changing sigma per iteration.\n",
      "            lr_decay (float): coefficient of changing learning rate per iteration.\n",
      "            percentile_growth (float): coefficient of changing percentile.\n",
      "            show_progress (bool): show training chart.\n",
      "            dV_angle (str): \"complanar\", \"collinear\" or \"auto\".\n",
      "            step_if_low_reward (bool): whether to step to the new table\n",
      "                if reward is lower than current or not.\n",
      "            early_stopping (bool): whether to stop training\n",
      "                if change of reward is negligibly small or not.\n",
      "\n",
      "        Returns:\n",
      "            stop (bool): whether to stop training after iteration.\n",
      "\n",
      "        TODO:\n",
      "            parallel\n",
      "            log\n",
      "            test\n",
      "\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# iteration docstring\n",
    "print(CE_model.iteration.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_parameters = {\n",
    "    \"n_sessions\": 30,\n",
    "    \"sigma_decay\": 0.98,\n",
    "    \"lr_decay\": 0.98,\n",
    "    \"percentile_growth\": 1.005,\n",
    "    \"show_progress\": False,\n",
    "    \"dV_angle\": 'complanar',\n",
    "    \"step_if_low_reward\": False,\n",
    "    \"early_stopping\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one iteration\n",
    "# CE_model.iteration(**iteration_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent policy (self.action_table).\n",
      "\n",
      "        Args:\n",
      "            n_iterations (int): number of iterations.\n",
      "            print_out (bool): print information during the training.\n",
      "            *args and **kwargs: iteration arguments, depend on method (inheritor class).\n",
      "\n",
      "        TODO:\n",
      "            add early stopping\n",
      "            add log\n",
      "            decorate by print_out and log?\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# train docstring\n",
    "print(CE_model.train.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 10\n",
    "print_out = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start training.\n",
      "\n",
      "Initial action table:\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.03333333]\n",
      " [0.         0.         0.                nan]]\n",
      "Initial Reward: -13489.530475182863\n",
      "\n",
      "iteration: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 92.64it/s]\n",
      " 30%|███       | 9/30 [00:00<00:00, 88.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Reward: -4334.831385075398\n",
      "Mean Reward:   -211.18501199231866\n",
      "Max Reward:    -58.81214384367388\n",
      "Threshold:     -59.51630430836516\n",
      "\n",
      "iteration: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 89.25it/s]\n",
      " 30%|███       | 9/30 [00:00<00:00, 83.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Reward: -4334.831385075398\n",
      "Mean Reward:   -954.0713019521978\n",
      "Max Reward:    -45.99339565139638\n",
      "Threshold:     -46.884056743425894\n",
      "\n",
      "iteration: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 82.28it/s]\n",
      " 27%|██▋       | 8/30 [00:00<00:00, 76.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Reward: -4334.831385075398\n",
      "Mean Reward:   -476.17674687232386\n",
      "Max Reward:    -58.37479359328544\n",
      "Threshold:     -69.41031537138508\n",
      "\n",
      "iteration: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 82.37it/s]\n",
      " 33%|███▎      | 10/30 [00:00<00:00, 91.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Reward: -1166.223965270826\n",
      "Mean Reward:   -863.589874523709\n",
      "Max Reward:    -6.378934485327951\n",
      "Threshold:     -42.35021561815753\n",
      "\n",
      "iteration: 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 86.70it/s]\n",
      " 30%|███       | 9/30 [00:00<00:00, 85.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Reward: -20.813393154452054\n",
      "Mean Reward:   -485.81189305686513\n",
      "Max Reward:    -23.509985662744135\n",
      "Threshold:     -34.96188717512047\n",
      "\n",
      "iteration: 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 83.59it/s]\n",
      " 33%|███▎      | 10/30 [00:00<00:00, 98.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Reward: -5.972422555274126\n",
      "Mean Reward:   -1165.8291152748013\n",
      "Max Reward:    -50.8567699230661\n",
      "Threshold:     -51.333481862693\n",
      "\n",
      "iteration: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 95.02it/s]\n",
      " 33%|███▎      | 10/30 [00:00<00:00, 92.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Reward: -5.972422555274126\n",
      "Mean Reward:   -310.48981825255373\n",
      "Max Reward:    -1.1999662744447743\n",
      "Threshold:     -16.045335484101777\n",
      "\n",
      "iteration: 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 95.84it/s]\n",
      " 33%|███▎      | 10/30 [00:00<00:00, 95.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Reward: -1.250985909188426\n",
      "Mean Reward:   -318.3544154687656\n",
      "Max Reward:    -5.3311370296320995\n",
      "Threshold:     -35.572154159373625\n",
      "\n",
      "iteration: 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 97.02it/s]\n",
      " 33%|███▎      | 10/30 [00:00<00:00, 94.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Reward: -1.250985909188426\n",
      "Mean Reward:   -243.46379630848887\n",
      "Max Reward:    -6.132162269411417\n",
      "Threshold:     -38.151115719375795\n",
      "\n",
      "iteration: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 96.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Reward: -1.250985909188426\n",
      "Mean Reward:   -424.6471472863572\n",
      "Max Reward:    -28.392408561735632\n",
      "Threshold:     -32.0333861449681\n",
      "\n",
      "Training completed in 3.4945 sec.\n",
      "Total Reward: -1.250985909188426\n",
      "Action Table:\n",
      "[[ 0.          0.          0.          0.        ]\n",
      " [ 1.28539765  0.70594096  0.01232225  0.07934858]\n",
      " [-1.28539765 -0.70594096 -0.01232225         nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CE_model.train(n_iterations, print_out, **iteration_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 1.28539765,  0.70594096,  0.01232225,  0.07934858],\n",
       "       [-1.28539765, -0.70594096, -0.01232225,         nan]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtained table of actions\n",
    "CE_model.action_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path =\n",
    "# CE_model.save_action_table(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_parameters = {\n",
    "    \"env\": env,\n",
    "    \"step\": 1e-6,\n",
    "    \"reverse\": True, \n",
    "    \"first_maneuver_time\": 'early',\n",
    "    \"n_maneuvers\": 2,\n",
    "    \"lr\": 0.9,\n",
    "    \"percentile\": 95,\n",
    "    \"sigma_dV\": None,\n",
    "    \"sigma_t\": None,\n",
    "}\n",
    "iteration_parameters = {\n",
    "    \"n_sessions\": 50,\n",
    "    \"sigma_decay\": 0.9,\n",
    "    \"lr_decay\": 0.9,\n",
    "    \"percentile_growth\": 1.01,\n",
    "    \"show_progress\": False,\n",
    "    \"dV_angle\": 'complanar',\n",
    "    \"step_if_low_reward\": False,\n",
    "    \"early_stopping\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_model = Baseline(env=env, step=1e-6, reverse=True)\n",
    "tune_CE_model = CrossEntropy(**init_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 109.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions table for tuning:\n",
      "[[ 0.          0.          0.          0.        ]\n",
      " [ 1.58325676  1.48626394  0.02594283  0.07934858]\n",
      " [-1.58325676 -1.48626394 -0.02594283         nan]]\n",
      "\n",
      "Reward: -0.7804488228736668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train init model\n",
    "init_model.train()\n",
    "# get obtained actions table\n",
    "init_action_table = init_model.action_table\n",
    "init_reward = init_model.policy_reward\n",
    "\n",
    "print(f\"Actions table for tuning:\\n{init_action_table}\")\n",
    "print(f\"\\nReward: {init_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 100.73it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 99.55it/s] \n",
      "100%|██████████| 50/50 [00:00<00:00, 96.84it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 93.76it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 92.19it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 90.24it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 93.47it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 94.92it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 94.72it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 101.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions table after tuning:\n",
      "[[ 0.          0.          0.          0.        ]\n",
      " [ 1.50530357  0.66501406  0.01160786  0.07934858]\n",
      " [-1.50530357 -0.66501406 -0.01160786         nan]]\n",
      "\n",
      "Reward: -0.6663004448026217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 10\n",
    "print_out = False\n",
    "\n",
    "# set initial actions table for tuning\n",
    "tune_CE_model.set_action_table(init_action_table)\n",
    "# tuning\n",
    "tune_CE_model.train(n_iterations, print_out, **iteration_parameters)\n",
    "\n",
    "print(f\"Actions table after tuning:\\n{tune_CE_model.action_table}\")\n",
    "print(f\"\\nReward: {tune_CE_model.policy_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
